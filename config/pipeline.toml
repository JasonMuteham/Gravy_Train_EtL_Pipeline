[pipeline]
name = "Gravy_Train_EtL_Pipeline"
description = "Data pipeline to analyse the expense claims of UK MP's"
# Database scheme name raw, staging, production etc
schema = "raw"
# Database engine local 'duckdb' or remote 'motherduck' 
database = "duckdb"
#database = "motherduck"

[logging]
level = "INFO"
log_folder = "log"
logfile = "gravy_train.log"

[task.get_mps_data]
active = false
description = "Call a custom function to extract MP & name history data from APIs"
file_type = "function.custom.get_mps_data"
param.mp_csv = "data/mp.csv"
param.name_history_folder = "data/name_history"

[task.get_constituency_data]
active = true
description = "Call a custom function to extract constituency & representation data from APIs"
file_type = "function.custom.get_constituency_data"
param.file_csv = "data/constituency.csv"
param.constituency_rep_folder = "data/constituency_representation"

[task.get_constituency_geometry_data]
active = false
description = "Call a custom function to extract constituency geometry data from APIs"
file_type = "function.custom.get_constituency_geometry_data"
param.file_csv = "data/constituency_geometry.csv"

[task.load_mps]
active = false
description = "Load MP data from csv file"
file_type = "csv"
url = "data/mp.csv"
sql_filter = ""
sql_table = "mp"
sql_write = "replace"

[task.load_expenses]
active = false
description = "Load MP data from csv file"
file_type = "csv"
url = "data/expenses/individual_business_costs/*.csv"
sql_filter = ""
sql_table = "expenses"
sql_write = "replace"

[task.load_name_history]
active = false
description = "Load MP name data from csv file"
file_type = "csv"
url = "data/name_history/*.csv"
sql_filter = ""
sql_table = "mp_names"
sql_write = "replace"

[task.date_table]
active = false
file_type = "csv"
description = "Date Dimension table"
url = "https://docs.google.com/spreadsheets/d/e/2PACX-1vRfi-0s1R6HmQMW2r_ywPQlpwSvpDVr3s0t6MJPgt88bmHR7R5Md0YWkHpeIj7NtuuSgt0vQiL66t1G/pub?gid=1535779642&single=true&output=csv"
sql_filter = ""
sql_table = "dim_dates"
sql_write = "replace"


[duckdb.credentials]
# local duckdb database name. Remote motherduck credentials are stored in secret.toml
path = "data/"
database = "gravy_train.duckdb"

[motherduck.credentials]
# local duckdb database name. Remote motherduck credentials are stored in secret.toml
database = "gravy_train_2"

#SQL Filter statements, filter the df_upload DataFrame
[sql.date_select]
sql = """
SELECT *
FROM df_upload
WHERE "date" >= '2020-01-01' and "date" < '2024-01-01'
"""