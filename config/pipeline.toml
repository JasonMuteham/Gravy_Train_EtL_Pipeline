[pipeline]
name = "Gravy_Train_EtL_Pipeline"
description = "Data pipeline to analyse the expense claims of UK MP's"
# Database scheme name raw, staging, production etc
schema = "raw"
# Database engine local 'duckdb' or remote 'motherduck' 
database = "duckdb"
#database = "motherduck"

[logging]
level = "INFO"
log_folder = "log"
logfile = "gravy_train.log"

[task.get_mps_expense_data]
active = true
description = "Call a custom function to extract MP data from APIs and all expenses data in .csv files"
file_type = "function.custom.get_mps_data"
param.mp_csv = "data/mp.csv"
param.expenses_csv = "data/expense_urls.csv"
param.expenses_folder = "data/expenses"

[task.load_mps]
active = false
description = "Load MP data from csv file"
file_type = "csv"
url = "data/mp.csv"
sql_filter = ""
sql_table = "mp"
sql_write = "replace"

[task.load_expenses]
active = false
description = "Load MP data from csv file"
file_type = "csv"
url = "data/expenses/*.csv"
sql_filter = ""
sql_table = "expenses"
sql_write = "replace"


[duckdb.credentials]
# local duckdb database name. Remote motherduck credentials are stored in secret.toml
path = "data/"
database = "gravy_train.duckdb"

[motherduck.credentials]
# local duckdb database name. Remote motherduck credentials are stored in secret.toml
database = "gravy_train_2"

#SQL Filter statements, filter the df_upload DataFrame
[sql.date_select]
sql = """
SELECT *
FROM df_upload
WHERE "date" >= '2020-01-01' and "date" < '2024-01-01'
"""